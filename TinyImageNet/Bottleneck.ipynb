{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca9113c-286e-4075-b71b-56250fbde461",
   "metadata": {},
   "source": [
    "<h1>Tiny ImageNet</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd9db28-cca1-47f5-8601-6e9e50b173e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "# Define the UnifiedStudentModel (Basic Block)\n",
    "class UnifiedStudentModel(nn.Module):\n",
    "    def __init__(self, vision_dim=256, teacher_output_dim=768, bottleneck_dim=128):\n",
    "        super(UnifiedStudentModel, self).__init__()\n",
    "        # Vision encoder with bottleneck\n",
    "        self.vision_encoder = nn.Sequential(\n",
    "            nn.Linear(teacher_output_dim, bottleneck_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(bottleneck_dim, vision_dim),\n",
    "        )\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * 0.07)\n",
    "\n",
    "    def forward(self, teacher_features):\n",
    "        # Normalize and project features\n",
    "        vision_proj = self.vision_encoder(teacher_features)\n",
    "        vision_proj = vision_proj / vision_proj.norm(dim=-1, keepdim=True)\n",
    "        logits = self.logit_scale.exp() * vision_proj @ vision_proj.t()\n",
    "        return logits\n",
    "\n",
    "# Initialize Tiny ImageNet dataset loaders\n",
    "def init_tiny_imagenet_data(data_dir, batch_size=32):\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dir = os.path.join(data_dir, \"train\")\n",
    "    train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "\n",
    "    val_dir = os.path.join(data_dir, \"val/images\")\n",
    "    val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Save and load checkpoint functions\n",
    "def save_checkpoint(student_model, optimizer, epoch, loss, checkpoint_dir, prefix):\n",
    "    if checkpoint_dir and prefix:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"{prefix}{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': student_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_dir, prefix, student_model, optimizer):\n",
    "    start_epoch = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "\n",
    "    if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    return start_epoch\n",
    "\n",
    "# Compute model size\n",
    "def compute_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024**2)\n",
    "\n",
    "data_dir = \"./data/tiny-imagenet-200\"\n",
    "train_loader, val_loader = init_tiny_imagenet_data(data_dir, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578347f-e7b0-442e-a42c-aeb4ad9fff59",
   "metadata": {},
   "source": [
    "<h2>BEiT</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9ca28-2506-4fa0-93d4-2879df0c5848",
   "metadata": {},
   "source": [
    "<h4>5 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875fd10c-937f-42ec-a8e9-f282e76e90ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize student model...\n",
      "Load teacher model...\n",
      "Training Student Model...\n",
      "Loading checkpoint from ./checkpoints/basic_block_beit_epoch_10.pt...\n",
      "Resuming training from epoch 10.\n",
      "Student Model Size: 0.50 MB\n",
      "Evaluating Student Model...\n",
      "Searching for checkpoints in ./checkpoints...\n",
      "Loading checkpoint from ./checkpoints/basic_block_beit_epoch_10.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5902/2002648508.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "/tmp/ipykernel_5902/3182499967.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Training function for the student model\n",
    "def train_student_model_beit(\n",
    "    teacher_model, student_model, train_loader, optimizer, \n",
    "    num_epochs=10, checkpoint_dir=None, prefix=None\n",
    "):\n",
    "    epoch_times = []\n",
    "    start_epoch = 0\n",
    "    if checkpoint_dir and prefix:\n",
    "        start_epoch = load_checkpoint(checkpoint_dir, prefix, student_model, optimizer)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Training epoch {epoch + 1}/{num_epochs}...\")\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Teacher model logits\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(images)\n",
    "                teacher_features = teacher_outputs.last_hidden_state.mean(dim=1)  # Global pooling\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features)\n",
    "\n",
    "            # Compute distillation loss\n",
    "            loss = torch.nn.functional.cross_entropy(student_logits, torch.arange(len(student_logits)).to(device))\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "        if checkpoint_dir and prefix:\n",
    "            save_checkpoint(student_model, optimizer, epoch, epoch_loss, checkpoint_dir, prefix)\n",
    "            \n",
    "    print(f\"Student Model Size: {compute_model_size(student_model):.2f} MB\")\n",
    "    return epoch_times\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_student_model_beit(student_model, teacher_model, val_loader, checkpoint_dir=None, prefix=None):\n",
    "    if checkpoint_dir and prefix:\n",
    "        print(f\"Searching for checkpoints in {checkpoint_dir}...\")\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    student_model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Teacher model logits\n",
    "            teacher_outputs = teacher_model(images)\n",
    "            teacher_features = teacher_outputs.last_hidden_state.mean(dim=1)  # Global pooling\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features)\n",
    "            predictions = torch.argmax(student_logits, dim=-1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().tolist())\n",
    "            all_labels.extend(torch.arange(len(predictions)).tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Initialize student model...\")\n",
    "# Initialize student model\n",
    "student_output_dim = 256\n",
    "teacher_output_dim = 768\n",
    "student_model = UnifiedStudentModel(vision_dim=student_output_dim, teacher_output_dim=teacher_output_dim)\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=3e-4)\n",
    "\n",
    "print(\"Load teacher model...\")\n",
    "# Load teacher model (DINO/BEiT)\n",
    "from transformers import BeitModel\n",
    "teacher_model = BeitModel.from_pretrained(\"microsoft/beit-base-patch16-224\").to('cuda')\n",
    "\n",
    "# Train the student model\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "prefix = \"basic_block_beit_epoch_\"\n",
    "\n",
    "print(\"Training Student Model...\")\n",
    "epoch_times = train_student_model_beit(\n",
    "    teacher_model, student_model, train_loader, optimizer, \n",
    "    num_epochs=5, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Evaluating Student Model...\")\n",
    "evaluate_student_model_beit(\n",
    "    student_model, teacher_model, val_loader, \n",
    "    checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae451a02-5d44-423b-a68b-17d00643ee5c",
   "metadata": {},
   "source": [
    "<h4>10 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd428b6-037c-4817-8265-a3c543998a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Student Model...\n",
      "Loading checkpoint from ./checkpoints/basic_block_beit_epoch_5.pt...\n",
      "Resuming training from epoch 5.\n",
      "Training epoch 6/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5540/2002648508.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Loss: 0.2207\n",
      "Epoch 6 completed in 1345.64 seconds.\n",
      "Checkpoint saved at ./checkpoints/basic_block_beit_epoch_6.pt\n",
      "Training epoch 7/10...\n",
      "Epoch 7/10 Loss: 0.2700\n",
      "Epoch 7 completed in 1344.25 seconds.\n",
      "Checkpoint saved at ./checkpoints/basic_block_beit_epoch_7.pt\n",
      "Training epoch 8/10...\n",
      "Epoch 8/10 Loss: 0.2281\n",
      "Epoch 8 completed in 1344.08 seconds.\n",
      "Checkpoint saved at ./checkpoints/basic_block_beit_epoch_8.pt\n",
      "Training epoch 9/10...\n",
      "Epoch 9/10 Loss: 0.2606\n",
      "Epoch 9 completed in 1344.90 seconds.\n",
      "Checkpoint saved at ./checkpoints/basic_block_beit_epoch_9.pt\n",
      "Training epoch 10/10...\n",
      "Epoch 10/10 Loss: 0.1026\n",
      "Epoch 10 completed in 1344.53 seconds.\n",
      "Checkpoint saved at ./checkpoints/basic_block_beit_epoch_10.pt\n",
      "Student Model Size: 0.50 MB\n",
      "Evaluating Student Model...\n",
      "Searching for checkpoints in ./checkpoints...\n",
      "Loading checkpoint from ./checkpoints/basic_block_beit_epoch_10.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5540/3182499967.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Student Model...\")\n",
    "epoch_times = train_student_model_beit(\n",
    "    teacher_model, student_model, train_loader, optimizer, \n",
    "    num_epochs=10, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Evaluating Student Model...\")\n",
    "evaluate_student_model_beit(\n",
    "    student_model, teacher_model, val_loader, \n",
    "    checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec573a8-8566-4fe9-aeaa-118b4193c80d",
   "metadata": {},
   "source": [
    "<h2>DINO</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d501c-1909-4353-825d-8379fb63422d",
   "metadata": {},
   "source": [
    "<h4>5 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef45b4cc-a76c-46eb-b56b-98899e002bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bottleneck Student Model...\n",
      "Loading DINO Teacher Model...\n",
      "Training Bottleneck Student Model...\n",
      "Loading checkpoint from ./checkpoints/bottleneck_dino_epoch_3.pt...\n",
      "Resuming training from epoch 3.\n",
      "Training epoch 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5580/2002648508.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Loss: 0.1621\n",
      "Epoch 4 completed in 744.66 seconds.\n",
      "Checkpoint saved at ./checkpoints/bottleneck_dino_epoch_4.pt\n",
      "Training epoch 5/5...\n",
      "Epoch 5/5 Loss: 0.1039\n",
      "Epoch 5 completed in 762.88 seconds.\n",
      "Checkpoint saved at ./checkpoints/bottleneck_dino_epoch_5.pt\n",
      "Student Model Size: 0.50 MB\n",
      "Evaluating Bottleneck Student Model...\n",
      "Searching for checkpoints in ./checkpoints...\n",
      "Loading checkpoint from ./checkpoints/bottleneck_dino_epoch_5.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5580/857233672.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Training function for the student model using DINO\n",
    "def train_student_model_dino(\n",
    "    teacher_model, student_model, train_loader, optimizer, \n",
    "    num_epochs=10, checkpoint_dir=None, prefix=None\n",
    "):\n",
    "    epoch_times = []\n",
    "    start_epoch = 0\n",
    "    if checkpoint_dir and prefix:\n",
    "        start_epoch = load_checkpoint(checkpoint_dir, prefix, student_model, optimizer)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Training epoch {epoch + 1}/{num_epochs}...\")\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Teacher model logits (DINO)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(images)\n",
    "                teacher_features = teacher_outputs[\"features\"].mean(dim=1)  # Global pooling\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features)\n",
    "\n",
    "            # Compute distillation loss\n",
    "            loss = torch.nn.functional.cross_entropy(student_logits, torch.arange(len(student_logits)).to(device))\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "        if checkpoint_dir and prefix:\n",
    "            save_checkpoint(student_model, optimizer, epoch, epoch_loss, checkpoint_dir, prefix)\n",
    "            \n",
    "    print(f\"Student Model Size: {compute_model_size(student_model):.2f} MB\")\n",
    "    return epoch_times\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_student_model_dino(student_model, teacher_model, val_loader, checkpoint_dir=None, prefix=None):\n",
    "    if checkpoint_dir and prefix:\n",
    "        print(f\"Searching for checkpoints in {checkpoint_dir}...\")\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    student_model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Teacher model logits (DINO)\n",
    "            teacher_outputs = teacher_model(images)\n",
    "            teacher_features = teacher_outputs[\"features\"].mean(dim=1)  # Global pooling\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features)\n",
    "            predictions = torch.argmax(student_logits, dim=-1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().tolist())\n",
    "            all_labels.extend(torch.arange(len(predictions)).tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Main Code to Train and Evaluate\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize Bottleneck Student Model\n",
    "print(\"Initializing Bottleneck Student Model...\")\n",
    "student_model = UnifiedStudentModel(vision_dim=256, teacher_output_dim=768, bottleneck_dim=128)\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=3e-4)\n",
    "\n",
    "# Load DINO Teacher Model\n",
    "print(\"Loading DINO Teacher Model...\")\n",
    "dino_teacher_model = vit_b_16(weights=\"IMAGENET1K_V1\").to('cuda')\n",
    "dino_teacher_model = create_feature_extractor(dino_teacher_model, return_nodes={\"encoder.layers\": \"features\"})\n",
    "\n",
    "# Load the latest checkpoint if it exists\n",
    "prefix = \"bottleneck_dino_epoch_\"\n",
    "\n",
    "# Train the student model\n",
    "print(\"Training Bottleneck Student Model...\")\n",
    "epoch_times = train_student_model_dino(\n",
    "    teacher_model=dino_teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs= 5,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    prefix=prefix\n",
    ")\n",
    "\n",
    "# Evaluate the student model\n",
    "print(\"Evaluating Bottleneck Student Model...\")\n",
    "evaluate_student_model_dino(\n",
    "    student_model=student_model,\n",
    "    teacher_model=dino_teacher_model,\n",
    "    val_loader=val_loader,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    prefix=prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54feb5fa-a27b-471a-9ccd-fd032ba712ba",
   "metadata": {},
   "source": [
    "<h4>10 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b11d35-0c55-4119-ad22-8abbbfe71cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Student Model with DINO...\")\n",
    "epoch_times = train_student_model_dino(\n",
    "    dino_teacher_model, student_model, train_loader, optimizer, \n",
    "    num_epochs=10, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Evaluating Student Model with DINO...\")\n",
    "evaluate_student_model_dino(\n",
    "    student_model, dino_teacher_model, val_loader, \n",
    "    checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deab322-4e2f-43e1-a8af-67fa524b5e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
