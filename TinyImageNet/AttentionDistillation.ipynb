{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f63392a-e71d-4a15-a36c-ea9f09d611ec",
   "metadata": {},
   "source": [
    "<h1>Tiny ImageNet</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd9db28-cca1-47f5-8601-6e9e50b173e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from transformers import BeitModel\n",
    "\n",
    "# Attention-based UnifiedStudentModel\n",
    "class UnifiedStudentModel(nn.Module):\n",
    "    def __init__(self, vision_dim=256, teacher_output_dim=768, num_heads=4):\n",
    "        super(UnifiedStudentModel, self).__init__()\n",
    "        # Vision attention\n",
    "        self.vision_attention = nn.MultiheadAttention(embed_dim=teacher_output_dim, num_heads=num_heads)\n",
    "        # Projection layer\n",
    "        self.vision_proj = nn.Linear(teacher_output_dim, vision_dim)\n",
    "        # Logit scale for similarity computation\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * 0.07)\n",
    "\n",
    "    def forward(self, vision_features):\n",
    "        \"\"\"\n",
    "        vision_features: Tensor of shape (batch_size, seq_len, teacher_output_dim)\n",
    "        \"\"\"\n",
    "        # Attention over vision features\n",
    "        vision_attn_output, _ = self.vision_attention(vision_features, vision_features, vision_features)\n",
    "        \n",
    "        # Project the attention output to vision dimension\n",
    "        vision_proj = self.vision_proj(vision_attn_output)\n",
    "        vision_proj = vision_proj / vision_proj.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute logits (self-similarity or pairwise similarity)\n",
    "        logits = self.logit_scale.exp() * vision_proj @ vision_proj.transpose(-1, -2)\n",
    "        return logits\n",
    "        \n",
    "# Initialize Tiny ImageNet dataset loaders\n",
    "def init_tiny_imagenet_data(data_dir, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Prepares the Tiny ImageNet dataset for training and validation.\n",
    "    \"\"\"\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Training dataset and loader\n",
    "    train_dir = os.path.join(data_dir, \"train\")\n",
    "    train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "\n",
    "    val_dir = os.path.join(data_dir, \"val/images\")\n",
    "    val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Save and load checkpoint functions\n",
    "def save_checkpoint(student_model, optimizer, epoch, loss, checkpoint_dir, prefix):\n",
    "    if checkpoint_dir and prefix:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"{prefix}{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': student_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_dir, prefix, student_model, optimizer):\n",
    "    start_epoch = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "\n",
    "    if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    return start_epoch\n",
    "\n",
    "# Compute model size\n",
    "def compute_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024**2)\n",
    "\n",
    "# Initialize Tiny ImageNet\n",
    "tiny_imagenet_dir = \"./data/tiny-imagenet-200\"\n",
    "train_loader, val_loader = init_tiny_imagenet_data(tiny_imagenet_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b327a-0795-4cf1-9c75-f74003b319d7",
   "metadata": {},
   "source": [
    "<h2>BEiT</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d0818-03dd-4ebf-8095-d78fe1601189",
   "metadata": {},
   "source": [
    "<h4>5 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe1bd18-3874-422c-95ff-a391d49ffbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_model_beit(teacher_model, student_model, train_loader, optimizer, num_epochs=10, checkpoint_dir=None, prefix=None):\n",
    "    \"\"\"\n",
    "    Trains the student model using features extracted from the BEiT teacher model.\n",
    "    \"\"\"\n",
    "    epoch_times = []\n",
    "    start_epoch = 0\n",
    "    if checkpoint_dir and prefix:\n",
    "        start_epoch = load_checkpoint(checkpoint_dir, prefix, student_model, optimizer)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Training epoch {epoch + 1}/{num_epochs}...\")\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Teacher model logits\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(images)\n",
    "                teacher_features = teacher_outputs.last_hidden_state  # Shape: [batch_size, seq_len, 768]\n",
    "                teacher_features = teacher_features.permute(1, 0, 2)  # Shape: [seq_len, batch_size, 768]\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features)\n",
    "\n",
    "            # Compute similarity logits and distillation loss\n",
    "            similarity_logits = student_logits @ student_logits.transpose(-1, -2)  # Shape: [seq_len, batch_size, batch_size]\n",
    "\n",
    "            # Create target tensor (diagonal of the similarity matrix)\n",
    "            targets = torch.arange(similarity_logits.size(1)).to(device)  # Shape: [batch_size]\n",
    "            targets = targets.unsqueeze(0).expand(similarity_logits.size(0), -1)  # Shape: [seq_len, batch_size]\n",
    "\n",
    "            # Cross-entropy loss\n",
    "            loss = F.cross_entropy(similarity_logits.reshape(-1, similarity_logits.size(-1)), targets.reshape(-1))\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "        if checkpoint_dir and prefix:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"{prefix}{epoch + 1}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': student_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_loss / len(train_loader)\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    print(f\"Student Model Size: {compute_model_size(student_model):.2f} MB\")\n",
    "    return epoch_times\n",
    "    \n",
    "def evaluate_student_model_beit(student_model, teacher_model, val_loader, checkpoint_dir=None, prefix=None):\n",
    "    \"\"\"\n",
    "    Evaluates the student model using features extracted from the BEiT teacher model.\n",
    "    \"\"\"\n",
    "    if checkpoint_dir and prefix:\n",
    "        print(f\"Searching for checkpoints in {checkpoint_dir}...\")\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    student_model.eval()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Teacher model logits\n",
    "            teacher_outputs = teacher_model(images)\n",
    "            teacher_features = teacher_outputs.last_hidden_state  # Shape: [batch_size, seq_len, 768]\n",
    "\n",
    "            # Project teacher features to match vision_dim\n",
    "            teacher_features = teacher_features.permute(1, 0, 2)  # [seq_len, batch_size, embed_dim]\n",
    "            student_logits = student_model(teacher_features)\n",
    "\n",
    "            # Compute similarity logits\n",
    "            similarity_logits = student_logits @ student_logits.transpose(-1, -2)\n",
    "\n",
    "            # Predictions and ground truth targets\n",
    "            predictions = torch.argmax(similarity_logits, dim=-1).flatten()\n",
    "            targets = torch.arange(similarity_logits.size(1)).to(device).repeat(similarity_logits.size(0))\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().tolist())\n",
    "            all_labels.extend(targets.cpu().tolist())\n",
    "\n",
    "    # Ensure correct format for sklearn accuracy_score\n",
    "    all_predictions = torch.tensor(all_predictions, dtype=torch.int64).flatten().tolist()\n",
    "    all_labels = torch.tensor(all_labels, dtype=torch.int64).flatten().tolist()\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Student model\n",
    "vision_dim = 256\n",
    "teacher_output_dim = 768\n",
    "num_heads = 4\n",
    "student_model = UnifiedStudentModel(vision_dim=vision_dim, teacher_output_dim=teacher_output_dim, num_heads=num_heads)\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=3e-4)\n",
    "\n",
    "# Load BEiT teacher model\n",
    "beit_teacher_model = BeitModel.from_pretrained(\"microsoft/beit-base-patch16-224\").to('cuda')\n",
    "\n",
    "# Training\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "prefix = \"attention_beit_epoch_\"\n",
    "\n",
    "print(\"Training Student Model with BEiT...\")\n",
    "epoch_times = train_student_model_beit(\n",
    "    beit_teacher_model, student_model, train_loader, optimizer,\n",
    "    num_epochs=5, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluating Student Model with BEiT...\")\n",
    "evaluate_student_model_beit(student_model, beit_teacher_model, val_loader, checkpoint_dir=checkpoint_dir, prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3351932-cb1f-4dfb-a140-435011f22a0e",
   "metadata": {},
   "source": [
    "<h4>10 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03637baf-fa49-49d0-98bc-0b078e5b0aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Student Model with BEiT...\n",
      "Loading checkpoint from ./checkpoints/attention_beit_epoch_5.pt...\n",
      "Resuming training from epoch 5.\n",
      "Training epoch 6/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5367/1685050101.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Loss: 0.3591\n",
      "Epoch 6 completed in 365.52 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention_beit_epoch_6.pt\n",
      "Training epoch 7/10...\n",
      "Epoch 7/10 Loss: 0.3589\n",
      "Epoch 7 completed in 364.49 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention_beit_epoch_7.pt\n",
      "Training epoch 8/10...\n",
      "Epoch 8/10 Loss: 0.3588\n",
      "Epoch 8 completed in 364.68 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention_beit_epoch_8.pt\n",
      "Training epoch 9/10...\n",
      "Epoch 9/10 Loss: 0.3573\n",
      "Epoch 9 completed in 364.70 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention_beit_epoch_9.pt\n",
      "Training epoch 10/10...\n",
      "Epoch 10/10 Loss: 0.3597\n",
      "Epoch 10 completed in 364.51 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention_beit_epoch_10.pt\n",
      "Student Model Size: 9.76 MB\n",
      "Evaluating Student Model with BEiT...\n",
      "Searching for checkpoints in ./checkpoints...\n",
      "Loading checkpoint from ./checkpoints/attention_beit_epoch_10.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5367/2988055231.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8450\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Student Model with BEiT...\")\n",
    "epoch_times = train_student_model_beit(\n",
    "    beit_teacher_model, student_model, train_loader, optimizer, \n",
    "    num_epochs=10, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluating Student Model with BEiT...\")\n",
    "evaluate_student_model_beit(student_model, beit_teacher_model, val_loader, checkpoint_dir=checkpoint_dir, prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec706753-6086-4693-b4c9-7169f48a9942",
   "metadata": {},
   "source": [
    "<h2>DINO</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cdc34-63b0-4c07-adfa-7bc25f9c98e0",
   "metadata": {},
   "source": [
    "<h4>5 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a35de7-51e0-4ea4-bd91-1d0e64077144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINO Teacher Model...\n",
      "Train Nodes: ['x', 'getattr', 'getitem', 'getitem_1', 'getitem_2', 'getitem_3', 'eq', '_assert', 'eq_1', '_assert_1', 'floordiv', 'floordiv_1', 'conv_proj', 'mul', 'reshape', 'permute', 'getattr_1', 'getitem_4', 'class_token', 'expand', 'cat', 'encoder.dim', 'encoder.eq', 'encoder.getattr', 'encoder._assert', 'encoder.encoder_pos_embedding', 'encoder.add', 'encoder.dropout', 'encoder.layers.encoder_layer_0.dim', 'encoder.layers.encoder_layer_0.eq', 'encoder.layers.encoder_layer_0.getattr', 'encoder.layers.encoder_layer_0._assert', 'encoder.layers.encoder_layer_0.ln', 'encoder.layers.encoder_layer_0.self_attention', 'encoder.layers.encoder_layer_0.getitem', 'encoder.layers.encoder_layer_0.getitem_1', 'encoder.layers.encoder_layer_0.dropout', 'encoder.layers.encoder_layer_0.add', 'encoder.layers.encoder_layer_0.ln_1', 'encoder.layers.encoder_layer_0.mlp', 'encoder.layers.encoder_layer_0.add_1', 'encoder.layers.encoder_layer_1.dim', 'encoder.layers.encoder_layer_1.eq', 'encoder.layers.encoder_layer_1.getattr', 'encoder.layers.encoder_layer_1._assert', 'encoder.layers.encoder_layer_1.ln', 'encoder.layers.encoder_layer_1.self_attention', 'encoder.layers.encoder_layer_1.getitem', 'encoder.layers.encoder_layer_1.getitem_1', 'encoder.layers.encoder_layer_1.dropout', 'encoder.layers.encoder_layer_1.add', 'encoder.layers.encoder_layer_1.ln_1', 'encoder.layers.encoder_layer_1.mlp', 'encoder.layers.encoder_layer_1.add_1', 'encoder.layers.encoder_layer_2.dim', 'encoder.layers.encoder_layer_2.eq', 'encoder.layers.encoder_layer_2.getattr', 'encoder.layers.encoder_layer_2._assert', 'encoder.layers.encoder_layer_2.ln', 'encoder.layers.encoder_layer_2.self_attention', 'encoder.layers.encoder_layer_2.getitem', 'encoder.layers.encoder_layer_2.getitem_1', 'encoder.layers.encoder_layer_2.dropout', 'encoder.layers.encoder_layer_2.add', 'encoder.layers.encoder_layer_2.ln_1', 'encoder.layers.encoder_layer_2.mlp', 'encoder.layers.encoder_layer_2.add_1', 'encoder.layers.encoder_layer_3.dim', 'encoder.layers.encoder_layer_3.eq', 'encoder.layers.encoder_layer_3.getattr', 'encoder.layers.encoder_layer_3._assert', 'encoder.layers.encoder_layer_3.ln', 'encoder.layers.encoder_layer_3.self_attention', 'encoder.layers.encoder_layer_3.getitem', 'encoder.layers.encoder_layer_3.getitem_1', 'encoder.layers.encoder_layer_3.dropout', 'encoder.layers.encoder_layer_3.add', 'encoder.layers.encoder_layer_3.ln_1', 'encoder.layers.encoder_layer_3.mlp', 'encoder.layers.encoder_layer_3.add_1', 'encoder.layers.encoder_layer_4.dim', 'encoder.layers.encoder_layer_4.eq', 'encoder.layers.encoder_layer_4.getattr', 'encoder.layers.encoder_layer_4._assert', 'encoder.layers.encoder_layer_4.ln', 'encoder.layers.encoder_layer_4.self_attention', 'encoder.layers.encoder_layer_4.getitem', 'encoder.layers.encoder_layer_4.getitem_1', 'encoder.layers.encoder_layer_4.dropout', 'encoder.layers.encoder_layer_4.add', 'encoder.layers.encoder_layer_4.ln_1', 'encoder.layers.encoder_layer_4.mlp', 'encoder.layers.encoder_layer_4.add_1', 'encoder.layers.encoder_layer_5.dim', 'encoder.layers.encoder_layer_5.eq', 'encoder.layers.encoder_layer_5.getattr', 'encoder.layers.encoder_layer_5._assert', 'encoder.layers.encoder_layer_5.ln', 'encoder.layers.encoder_layer_5.self_attention', 'encoder.layers.encoder_layer_5.getitem', 'encoder.layers.encoder_layer_5.getitem_1', 'encoder.layers.encoder_layer_5.dropout', 'encoder.layers.encoder_layer_5.add', 'encoder.layers.encoder_layer_5.ln_1', 'encoder.layers.encoder_layer_5.mlp', 'encoder.layers.encoder_layer_5.add_1', 'encoder.layers.encoder_layer_6.dim', 'encoder.layers.encoder_layer_6.eq', 'encoder.layers.encoder_layer_6.getattr', 'encoder.layers.encoder_layer_6._assert', 'encoder.layers.encoder_layer_6.ln', 'encoder.layers.encoder_layer_6.self_attention', 'encoder.layers.encoder_layer_6.getitem', 'encoder.layers.encoder_layer_6.getitem_1', 'encoder.layers.encoder_layer_6.dropout', 'encoder.layers.encoder_layer_6.add', 'encoder.layers.encoder_layer_6.ln_1', 'encoder.layers.encoder_layer_6.mlp', 'encoder.layers.encoder_layer_6.add_1', 'encoder.layers.encoder_layer_7.dim', 'encoder.layers.encoder_layer_7.eq', 'encoder.layers.encoder_layer_7.getattr', 'encoder.layers.encoder_layer_7._assert', 'encoder.layers.encoder_layer_7.ln', 'encoder.layers.encoder_layer_7.self_attention', 'encoder.layers.encoder_layer_7.getitem', 'encoder.layers.encoder_layer_7.getitem_1', 'encoder.layers.encoder_layer_7.dropout', 'encoder.layers.encoder_layer_7.add', 'encoder.layers.encoder_layer_7.ln_1', 'encoder.layers.encoder_layer_7.mlp', 'encoder.layers.encoder_layer_7.add_1', 'encoder.layers.encoder_layer_8.dim', 'encoder.layers.encoder_layer_8.eq', 'encoder.layers.encoder_layer_8.getattr', 'encoder.layers.encoder_layer_8._assert', 'encoder.layers.encoder_layer_8.ln', 'encoder.layers.encoder_layer_8.self_attention', 'encoder.layers.encoder_layer_8.getitem', 'encoder.layers.encoder_layer_8.getitem_1', 'encoder.layers.encoder_layer_8.dropout', 'encoder.layers.encoder_layer_8.add', 'encoder.layers.encoder_layer_8.ln_1', 'encoder.layers.encoder_layer_8.mlp', 'encoder.layers.encoder_layer_8.add_1', 'encoder.layers.encoder_layer_9.dim', 'encoder.layers.encoder_layer_9.eq', 'encoder.layers.encoder_layer_9.getattr', 'encoder.layers.encoder_layer_9._assert', 'encoder.layers.encoder_layer_9.ln', 'encoder.layers.encoder_layer_9.self_attention', 'encoder.layers.encoder_layer_9.getitem', 'encoder.layers.encoder_layer_9.getitem_1', 'encoder.layers.encoder_layer_9.dropout', 'encoder.layers.encoder_layer_9.add', 'encoder.layers.encoder_layer_9.ln_1', 'encoder.layers.encoder_layer_9.mlp', 'encoder.layers.encoder_layer_9.add_1', 'encoder.layers.encoder_layer_10.dim', 'encoder.layers.encoder_layer_10.eq', 'encoder.layers.encoder_layer_10.getattr', 'encoder.layers.encoder_layer_10._assert', 'encoder.layers.encoder_layer_10.ln', 'encoder.layers.encoder_layer_10.self_attention', 'encoder.layers.encoder_layer_10.getitem', 'encoder.layers.encoder_layer_10.getitem_1', 'encoder.layers.encoder_layer_10.dropout', 'encoder.layers.encoder_layer_10.add', 'encoder.layers.encoder_layer_10.ln_1', 'encoder.layers.encoder_layer_10.mlp', 'encoder.layers.encoder_layer_10.add_1', 'encoder.layers.encoder_layer_11.dim', 'encoder.layers.encoder_layer_11.eq', 'encoder.layers.encoder_layer_11.getattr', 'encoder.layers.encoder_layer_11._assert', 'encoder.layers.encoder_layer_11.ln', 'encoder.layers.encoder_layer_11.self_attention', 'encoder.layers.encoder_layer_11.getitem', 'encoder.layers.encoder_layer_11.getitem_1', 'encoder.layers.encoder_layer_11.dropout', 'encoder.layers.encoder_layer_11.add', 'encoder.layers.encoder_layer_11.ln_1', 'encoder.layers.encoder_layer_11.mlp', 'encoder.layers.encoder_layer_11.add_1', 'encoder.ln', 'getitem_5', 'heads.head']\n",
      "Eval Nodes: ['x', 'getattr', 'getitem', 'getitem_1', 'getitem_2', 'getitem_3', 'eq', '_assert', 'eq_1', '_assert_1', 'floordiv', 'floordiv_1', 'conv_proj', 'mul', 'reshape', 'permute', 'getattr_1', 'getitem_4', 'class_token', 'expand', 'cat', 'encoder.dim', 'encoder.eq', 'encoder.getattr', 'encoder._assert', 'encoder.encoder_pos_embedding', 'encoder.add', 'encoder.dropout', 'encoder.layers.encoder_layer_0.dim', 'encoder.layers.encoder_layer_0.eq', 'encoder.layers.encoder_layer_0.getattr', 'encoder.layers.encoder_layer_0._assert', 'encoder.layers.encoder_layer_0.ln', 'encoder.layers.encoder_layer_0.self_attention', 'encoder.layers.encoder_layer_0.getitem', 'encoder.layers.encoder_layer_0.getitem_1', 'encoder.layers.encoder_layer_0.dropout', 'encoder.layers.encoder_layer_0.add', 'encoder.layers.encoder_layer_0.ln_1', 'encoder.layers.encoder_layer_0.mlp', 'encoder.layers.encoder_layer_0.add_1', 'encoder.layers.encoder_layer_1.dim', 'encoder.layers.encoder_layer_1.eq', 'encoder.layers.encoder_layer_1.getattr', 'encoder.layers.encoder_layer_1._assert', 'encoder.layers.encoder_layer_1.ln', 'encoder.layers.encoder_layer_1.self_attention', 'encoder.layers.encoder_layer_1.getitem', 'encoder.layers.encoder_layer_1.getitem_1', 'encoder.layers.encoder_layer_1.dropout', 'encoder.layers.encoder_layer_1.add', 'encoder.layers.encoder_layer_1.ln_1', 'encoder.layers.encoder_layer_1.mlp', 'encoder.layers.encoder_layer_1.add_1', 'encoder.layers.encoder_layer_2.dim', 'encoder.layers.encoder_layer_2.eq', 'encoder.layers.encoder_layer_2.getattr', 'encoder.layers.encoder_layer_2._assert', 'encoder.layers.encoder_layer_2.ln', 'encoder.layers.encoder_layer_2.self_attention', 'encoder.layers.encoder_layer_2.getitem', 'encoder.layers.encoder_layer_2.getitem_1', 'encoder.layers.encoder_layer_2.dropout', 'encoder.layers.encoder_layer_2.add', 'encoder.layers.encoder_layer_2.ln_1', 'encoder.layers.encoder_layer_2.mlp', 'encoder.layers.encoder_layer_2.add_1', 'encoder.layers.encoder_layer_3.dim', 'encoder.layers.encoder_layer_3.eq', 'encoder.layers.encoder_layer_3.getattr', 'encoder.layers.encoder_layer_3._assert', 'encoder.layers.encoder_layer_3.ln', 'encoder.layers.encoder_layer_3.self_attention', 'encoder.layers.encoder_layer_3.getitem', 'encoder.layers.encoder_layer_3.getitem_1', 'encoder.layers.encoder_layer_3.dropout', 'encoder.layers.encoder_layer_3.add', 'encoder.layers.encoder_layer_3.ln_1', 'encoder.layers.encoder_layer_3.mlp', 'encoder.layers.encoder_layer_3.add_1', 'encoder.layers.encoder_layer_4.dim', 'encoder.layers.encoder_layer_4.eq', 'encoder.layers.encoder_layer_4.getattr', 'encoder.layers.encoder_layer_4._assert', 'encoder.layers.encoder_layer_4.ln', 'encoder.layers.encoder_layer_4.self_attention', 'encoder.layers.encoder_layer_4.getitem', 'encoder.layers.encoder_layer_4.getitem_1', 'encoder.layers.encoder_layer_4.dropout', 'encoder.layers.encoder_layer_4.add', 'encoder.layers.encoder_layer_4.ln_1', 'encoder.layers.encoder_layer_4.mlp', 'encoder.layers.encoder_layer_4.add_1', 'encoder.layers.encoder_layer_5.dim', 'encoder.layers.encoder_layer_5.eq', 'encoder.layers.encoder_layer_5.getattr', 'encoder.layers.encoder_layer_5._assert', 'encoder.layers.encoder_layer_5.ln', 'encoder.layers.encoder_layer_5.self_attention', 'encoder.layers.encoder_layer_5.getitem', 'encoder.layers.encoder_layer_5.getitem_1', 'encoder.layers.encoder_layer_5.dropout', 'encoder.layers.encoder_layer_5.add', 'encoder.layers.encoder_layer_5.ln_1', 'encoder.layers.encoder_layer_5.mlp', 'encoder.layers.encoder_layer_5.add_1', 'encoder.layers.encoder_layer_6.dim', 'encoder.layers.encoder_layer_6.eq', 'encoder.layers.encoder_layer_6.getattr', 'encoder.layers.encoder_layer_6._assert', 'encoder.layers.encoder_layer_6.ln', 'encoder.layers.encoder_layer_6.self_attention', 'encoder.layers.encoder_layer_6.getitem', 'encoder.layers.encoder_layer_6.getitem_1', 'encoder.layers.encoder_layer_6.dropout', 'encoder.layers.encoder_layer_6.add', 'encoder.layers.encoder_layer_6.ln_1', 'encoder.layers.encoder_layer_6.mlp', 'encoder.layers.encoder_layer_6.add_1', 'encoder.layers.encoder_layer_7.dim', 'encoder.layers.encoder_layer_7.eq', 'encoder.layers.encoder_layer_7.getattr', 'encoder.layers.encoder_layer_7._assert', 'encoder.layers.encoder_layer_7.ln', 'encoder.layers.encoder_layer_7.self_attention', 'encoder.layers.encoder_layer_7.getitem', 'encoder.layers.encoder_layer_7.getitem_1', 'encoder.layers.encoder_layer_7.dropout', 'encoder.layers.encoder_layer_7.add', 'encoder.layers.encoder_layer_7.ln_1', 'encoder.layers.encoder_layer_7.mlp', 'encoder.layers.encoder_layer_7.add_1', 'encoder.layers.encoder_layer_8.dim', 'encoder.layers.encoder_layer_8.eq', 'encoder.layers.encoder_layer_8.getattr', 'encoder.layers.encoder_layer_8._assert', 'encoder.layers.encoder_layer_8.ln', 'encoder.layers.encoder_layer_8.self_attention', 'encoder.layers.encoder_layer_8.getitem', 'encoder.layers.encoder_layer_8.getitem_1', 'encoder.layers.encoder_layer_8.dropout', 'encoder.layers.encoder_layer_8.add', 'encoder.layers.encoder_layer_8.ln_1', 'encoder.layers.encoder_layer_8.mlp', 'encoder.layers.encoder_layer_8.add_1', 'encoder.layers.encoder_layer_9.dim', 'encoder.layers.encoder_layer_9.eq', 'encoder.layers.encoder_layer_9.getattr', 'encoder.layers.encoder_layer_9._assert', 'encoder.layers.encoder_layer_9.ln', 'encoder.layers.encoder_layer_9.self_attention', 'encoder.layers.encoder_layer_9.getitem', 'encoder.layers.encoder_layer_9.getitem_1', 'encoder.layers.encoder_layer_9.dropout', 'encoder.layers.encoder_layer_9.add', 'encoder.layers.encoder_layer_9.ln_1', 'encoder.layers.encoder_layer_9.mlp', 'encoder.layers.encoder_layer_9.add_1', 'encoder.layers.encoder_layer_10.dim', 'encoder.layers.encoder_layer_10.eq', 'encoder.layers.encoder_layer_10.getattr', 'encoder.layers.encoder_layer_10._assert', 'encoder.layers.encoder_layer_10.ln', 'encoder.layers.encoder_layer_10.self_attention', 'encoder.layers.encoder_layer_10.getitem', 'encoder.layers.encoder_layer_10.getitem_1', 'encoder.layers.encoder_layer_10.dropout', 'encoder.layers.encoder_layer_10.add', 'encoder.layers.encoder_layer_10.ln_1', 'encoder.layers.encoder_layer_10.mlp', 'encoder.layers.encoder_layer_10.add_1', 'encoder.layers.encoder_layer_11.dim', 'encoder.layers.encoder_layer_11.eq', 'encoder.layers.encoder_layer_11.getattr', 'encoder.layers.encoder_layer_11._assert', 'encoder.layers.encoder_layer_11.ln', 'encoder.layers.encoder_layer_11.self_attention', 'encoder.layers.encoder_layer_11.getitem', 'encoder.layers.encoder_layer_11.getitem_1', 'encoder.layers.encoder_layer_11.dropout', 'encoder.layers.encoder_layer_11.add', 'encoder.layers.encoder_layer_11.ln_1', 'encoder.layers.encoder_layer_11.mlp', 'encoder.layers.encoder_layer_11.add_1', 'encoder.ln', 'getitem_5', 'heads.head']\n",
      "Training Attention-based Student Model...\n",
      "Training epoch 1/5...\n",
      "Epoch 1/5 Loss: 1.7730\n",
      "Epoch 1 completed in 335.16 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_1.pt\n",
      "Training epoch 2/5...\n",
      "Epoch 2/5 Loss: 1.3276\n",
      "Epoch 2 completed in 358.35 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_2.pt\n",
      "Training epoch 3/5...\n",
      "Epoch 3/5 Loss: 1.2294\n",
      "Epoch 3 completed in 334.27 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_3.pt\n",
      "Training epoch 4/5...\n",
      "Epoch 4/5 Loss: 1.2178\n",
      "Epoch 4 completed in 380.29 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_4.pt\n",
      "Training epoch 5/5...\n",
      "Epoch 5/5 Loss: 1.2069\n",
      "Epoch 5 completed in 506.80 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_5.pt\n",
      "Student Model Size: 9.76 MB\n",
      "Evaluating Attention-based Student Model...\n",
      "Searching for checkpoints in ./checkpoints...\n",
      "Loading checkpoint from ./checkpoints/attention2_dino_epoch_5.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5429/1541162988.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.2004\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "# Attention-based UnifiedStudentModel\n",
    "class UnifiedStudentModel(nn.Module):\n",
    "    def __init__(self, vision_dim=256, teacher_output_dim=768, num_heads=4):\n",
    "        super(UnifiedStudentModel, self).__init__()\n",
    "        # Vision attention\n",
    "        self.vision_attention = nn.MultiheadAttention(embed_dim=teacher_output_dim, num_heads=num_heads)\n",
    "        # Projection layer\n",
    "        self.vision_proj = nn.Linear(teacher_output_dim, vision_dim)\n",
    "        # Logit scale for similarity computation\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * 0.07)\n",
    "\n",
    "    def forward(self, vision_features):\n",
    "        \"\"\"\n",
    "        vision_features: Tensor of shape (batch_size, seq_len, teacher_output_dim)\n",
    "        \"\"\"\n",
    "        # Attention over vision features\n",
    "        vision_attn_output, _ = self.vision_attention(vision_features, vision_features, vision_features)\n",
    "        \n",
    "        # Project the attention output to vision dimension\n",
    "        vision_proj = self.vision_proj(vision_attn_output)\n",
    "        vision_proj = vision_proj / vision_proj.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute logits (self-similarity or pairwise similarity)\n",
    "        logits = self.logit_scale.exp() * vision_proj @ vision_proj.transpose(-1, -2)\n",
    "        return logits\n",
    "        \n",
    "# Initialize Tiny ImageNet dataset loaders\n",
    "def init_tiny_imagenet_data(data_dir, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Prepares the Tiny ImageNet dataset for training and validation.\n",
    "    \"\"\"\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Training dataset and loader\n",
    "    train_dir = os.path.join(data_dir, \"train\")\n",
    "    train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "\n",
    "    val_dir = os.path.join(data_dir, \"val/images\")\n",
    "    val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Save and load checkpoint functions\n",
    "def save_checkpoint(student_model, optimizer, epoch, loss, checkpoint_dir, prefix):\n",
    "    if checkpoint_dir and prefix:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"{prefix}{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': student_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_dir, prefix, student_model, optimizer):\n",
    "    start_epoch = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "\n",
    "    if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    return start_epoch\n",
    "\n",
    "# Compute model size\n",
    "def compute_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024**2)\n",
    "\n",
    "# Initialize Tiny ImageNet\n",
    "tiny_imagenet_dir = \"./data/tiny-imagenet-200\"\n",
    "train_loader, val_loader = init_tiny_imagenet_data(tiny_imagenet_dir)\n",
    "\n",
    "def train_student_model_dino(teacher_model, student_model, train_loader, optimizer, num_epochs=10, checkpoint_dir=None, prefix=None):\n",
    "    \"\"\"\n",
    "    Trains the student model using features extracted from the DINO teacher model.\n",
    "    \"\"\"\n",
    "    epoch_times = []\n",
    "    start_epoch = 0\n",
    "    if checkpoint_dir and prefix:\n",
    "        start_epoch = load_checkpoint(checkpoint_dir, prefix, student_model, optimizer)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Training epoch {epoch + 1}/{num_epochs}...\")\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Teacher model logits (DINO)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(images)\n",
    "                teacher_features = teacher_outputs[\"features\"]  # Use pooled features\n",
    "                teacher_features = F.normalize(teacher_features, p=2, dim=-1)  # Normalize features\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features.unsqueeze(0))  # Add sequence dimension\n",
    "\n",
    "            # Compute similarity logits\n",
    "            similarity_logits = student_logits @ student_logits.transpose(-1, -2)\n",
    "            similarity_logits /= torch.sqrt(torch.tensor(student_logits.size(-1), dtype=torch.float32, device=device))\n",
    "\n",
    "            # Ensure logits shape matches [batch_size, batch_size]\n",
    "            similarity_logits = similarity_logits.squeeze(0)\n",
    "\n",
    "            # Create target tensor\n",
    "            targets = torch.arange(similarity_logits.size(0)).to(device)  # [batch_size]\n",
    "\n",
    "            # Cross-entropy loss\n",
    "            loss = F.cross_entropy(similarity_logits, targets)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "        if checkpoint_dir and prefix:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"{prefix}{epoch + 1}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': student_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_loss / len(train_loader)\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    print(f\"Student Model Size: {compute_model_size(student_model):.2f} MB\")\n",
    "    return epoch_times\n",
    "\n",
    "# Adjust feature extractor for DINO\n",
    "print(\"Loading DINO Teacher Model...\")\n",
    "dino_teacher_model = vit_b_16(weights=\"IMAGENET1K_V1\").to('cuda')\n",
    "train_nodes, eval_nodes = get_graph_node_names(dino_teacher_model)\n",
    "print(\"Train Nodes:\", train_nodes)\n",
    "print(\"Eval Nodes:\", eval_nodes)\n",
    "dino_teacher_model = create_feature_extractor(\n",
    "    dino_teacher_model, \n",
    "    return_nodes={\"getitem_5\": \"features\"}\n",
    ")\n",
    "\n",
    "# Initialize optimizer and student model\n",
    "student_model = UnifiedStudentModel(vision_dim=256, teacher_output_dim=768, num_heads=4)\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "# Training\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "prefix = \"attention2_dino_epoch_\"\n",
    "\n",
    "print(\"Training Attention-based Student Model...\")\n",
    "epoch_times = train_student_model_dino(\n",
    "    teacher_model=dino_teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=5,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    prefix=prefix\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_student_model_dino(student_model, teacher_model, val_loader, checkpoint_dir=None, prefix=None):\n",
    "    \"\"\"\n",
    "    Evaluates the student model using features extracted from the DINO teacher model.\n",
    "    \"\"\"\n",
    "    if checkpoint_dir:\n",
    "        print(f\"Searching for checkpoints in {checkpoint_dir}...\")\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    student_model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            teacher_outputs = teacher_model(images)\n",
    "            teacher_features = teacher_outputs[\"features\"]  # Global pooling\n",
    "            teacher_features = F.normalize(teacher_features, p=2, dim=-1)  # Normalize features\n",
    "\n",
    "            # Student logits\n",
    "            student_logits = student_model(teacher_features.unsqueeze(0))\n",
    "            similarity_logits = student_logits @ student_logits.transpose(-1, -2)\n",
    "            similarity_logits /= torch.sqrt(torch.tensor(student_logits.size(-1), dtype=torch.float32, device=device))\n",
    "\n",
    "            # Generate predictions and targets\n",
    "            predictions = torch.argmax(similarity_logits, dim=1).flatten()\n",
    "            batch_size = images.size(0)  # Ensure targets match batch size\n",
    "            targets = torch.arange(batch_size).to(device).flatten()\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Evaluating Attention-based Student Model...\")\n",
    "evaluate_student_model_dino(\n",
    "    student_model=student_model,\n",
    "    teacher_model=dino_teacher_model,\n",
    "    val_loader=val_loader,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    prefix=prefix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f861a-de5f-4932-b87e-06f0daf85487",
   "metadata": {},
   "source": [
    "<h4>10 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44d55a4-0018-498e-903e-2f80b1caba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Attention-based Student Model...\n",
      "Loading checkpoint from ./checkpoints/attention2_dino_epoch_5.pt...\n",
      "Resuming training from epoch 5.\n",
      "Training epoch 6/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5429/1541162988.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Loss: 1.2058\n",
      "Epoch 6 completed in 877.40 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_6.pt\n",
      "Training epoch 7/10...\n",
      "Epoch 7/10 Loss: 1.1912\n",
      "Epoch 7 completed in 877.39 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_7.pt\n",
      "Training epoch 8/10...\n",
      "Epoch 8/10 Loss: 1.2033\n",
      "Epoch 8 completed in 877.98 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_8.pt\n",
      "Training epoch 9/10...\n",
      "Epoch 9/10 Loss: 1.2065\n",
      "Epoch 9 completed in 875.80 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_9.pt\n",
      "Training epoch 10/10...\n",
      "Epoch 10/10 Loss: 1.1927\n",
      "Epoch 10 completed in 877.62 seconds.\n",
      "Checkpoint saved at ./checkpoints/attention2_dino_epoch_10.pt\n",
      "Student Model Size: 9.76 MB\n",
      "Evaluating Attention-based Student Model...\n",
      "Searching for checkpoints in ./checkpoints...\n",
      "Loading checkpoint from ./checkpoints/attention2_dino_epoch_10.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5429/1541162988.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.2004\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Attention-based Student Model...\")\n",
    "epoch_times = train_student_model_dino(\n",
    "    teacher_model=dino_teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=10,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Evaluating Attention-based Student Model...\")\n",
    "evaluate_student_model_dino(\n",
    "    student_model=student_model,\n",
    "    teacher_model=dino_teacher_model,\n",
    "    val_loader=val_loader,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    prefix=prefix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef594eee-bfcd-46cc-8603-0eda552453c3",
   "metadata": {},
   "source": [
    "<h1>COCO</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901a559e-8b16-43ae-94f3-9e760631652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=18.73s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=2.61s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# COCO dataset loader for classification\n",
    "class COCOClassification(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation_file, transform=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.transform = transform\n",
    "        self.image_ids = list(self.coco.imgToAnns.keys())\n",
    "        self.classes = list(self.coco.cats.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        annotations = self.coco.loadAnns(self.coco.getAnnIds(imgIds=image_id))\n",
    "\n",
    "        # Load image\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root, image_info['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Create multi-label vector\n",
    "        labels = [0] * len(self.classes)\n",
    "        for ann in annotations:\n",
    "            category_idx = self.classes.index(ann['category_id'])\n",
    "            labels[category_idx] = 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# Save checkpoint\n",
    "def save_checkpoint(student_model, optimizer, epoch, loss, checkpoint_dir, prefix):\n",
    "    if checkpoint_dir and prefix:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"{prefix}{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': student_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Load checkpoint\n",
    "def load_checkpoint(checkpoint_dir, prefix, student_model, optimizer=None):\n",
    "    start_epoch = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "\n",
    "    if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = [\n",
    "            f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = max(\n",
    "                checkpoint_files,\n",
    "                key=lambda x: int(x[len(prefix):-3])\n",
    "            )\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            if 'epoch' in checkpoint:\n",
    "                start_epoch = checkpoint['epoch']\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    return start_epoch\n",
    "    \n",
    "# Initialize COCO dataset loaders\n",
    "def init_coco_data(data_dir, annotation_file, batch_size=32):\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = COCOClassification(root=data_dir, annotation_file=annotation_file, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "# Student Model\n",
    "def compute_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024**2)\n",
    "\n",
    "class SimpleProjectionModel(nn.Module):\n",
    "    def __init__(self, vision_dim=256, num_classes=80):\n",
    "        super(SimpleProjectionModel, self).__init__()\n",
    "        self.vision_encoder = nn.Linear(vision_dim, num_classes)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * 0.07)\n",
    "\n",
    "    def forward(self, teacher_features):\n",
    "        logits = self.vision_encoder(teacher_features)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Paths for COCO dataset\n",
    "train_data_dir = \"/home/yx3493/train2017/train2017\"\n",
    "train_annotation_file = \"/home/yx3493/annotations_trainval2017/annotations/instances_train2017.json\"\n",
    "val_data_dir = \"/home/yx3493/val2017/val2017\"\n",
    "val_annotation_file = \"/home/yx3493/annotations_trainval2017/annotations/instances_val2017.json\"\n",
    "\n",
    "train_loader = init_coco_data(train_data_dir, train_annotation_file, batch_size=8)\n",
    "val_loader = init_coco_data(val_data_dir, val_annotation_file, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293a5d2-20ff-40cb-b08f-0253348e4efe",
   "metadata": {},
   "source": [
    "<h2>BEiT</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a34ae-ef92-4e92-944a-ff21253f84f1",
   "metadata": {},
   "source": [
    "<h4>5 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05d216a-c864-4fb3-a3c0-c1275789816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Student Model with COCO dataset...\n",
      "Loading checkpoint from ./checkpoints/attention_beit_coco_epoch_2.pt...\n",
      "Resuming training from epoch 2.\n",
      "Training epoch 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yx3493/.local/lib/python3.9/site-packages/transformers/models/beit/feature_extraction_beit.py:28: FutureWarning: The class BeitFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use BeitImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/yx3493/.local/lib/python3.9/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `BeitFeatureExtractor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "/tmp/ipykernel_12058/3170219303.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Loss: 1093.0803\n",
      "Checkpoint saved at ./checkpoints/attention_beit_coco_epoch_3.pt\n",
      "Training epoch 4/5...\n",
      "Epoch 4/5 Loss: 1090.1349\n",
      "Checkpoint saved at ./checkpoints/attention_beit_coco_epoch_4.pt\n",
      "Training epoch 5/5...\n",
      "Epoch 5/5 Loss: 1088.9544\n",
      "Checkpoint saved at ./checkpoints/attention_beit_coco_epoch_5.pt\n",
      "Student Model Size: 0.23 MB\n",
      "Training complete.\n",
      "Evaluating Student Model...\n",
      "Loading checkpoint from ./checkpoints/attention_beit_coco_epoch_5.pt...\n",
      "Resuming training from epoch 5.\n",
      "Evaluation Accuracy: 0.2161\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Training logic\n",
    "def train_student_model_beit(\n",
    "    teacher_model, teacher_processor, student_model, train_loader, optimizer,\n",
    "    num_epochs=10, checkpoint_dir=None, prefix=None\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "    start_epoch = load_checkpoint(checkpoint_dir, prefix, student_model, optimizer)\n",
    "\n",
    "    # Multi-label loss\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Training epoch {epoch + 1}/{num_epochs}...\")\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            pil_images = [transforms.ToPILImage()(img) for img in images]\n",
    "\n",
    "            # Teacher model logits\n",
    "            with torch.no_grad():\n",
    "                teacher_inputs = teacher_processor(images=pil_images, return_tensors=\"pt\").to(device)\n",
    "                teacher_outputs = teacher_model(**teacher_inputs)\n",
    "                teacher_features = teacher_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features)\n",
    "\n",
    "            # Compute loss (multi-label)\n",
    "            loss = criterion(student_logits, labels.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss:.4f}\")\n",
    "        save_checkpoint(student_model, optimizer, epoch, epoch_loss, checkpoint_dir, prefix)\n",
    "    print(f\"Student Model Size: {compute_model_size(student_model):.2f} MB\")\n",
    "\n",
    "# Evaluation logic\n",
    "def evaluate_student_model_beit(\n",
    "    student_model, teacher_model, teacher_processor, val_loader, checkpoint_dir=None, prefix=None\n",
    "):\n",
    "    start_epoch = load_checkpoint(checkpoint_dir, prefix, student_model, optimizer=None)\n",
    "    student_model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    student_model.to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            pil_images = [transforms.ToPILImage()(img) for img in images]\n",
    "\n",
    "            # Teacher model logits\n",
    "            teacher_inputs = teacher_processor(images=pil_images, return_tensors=\"pt\").to(device)\n",
    "            teacher_outputs = teacher_model(**teacher_inputs)\n",
    "            teacher_features = teacher_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "            # Student model logits\n",
    "            student_logits = student_model(teacher_features)\n",
    "            predictions = torch.sigmoid(student_logits) > 0.5  # Multi-label thresholding\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute multi-label accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        [tuple(map(int, x)) for x in all_labels], [tuple(map(int, x)) for x in all_predictions]\n",
    "    )\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Initialize teacher and student models\n",
    "from transformers import BeitModel, BeitFeatureExtractor\n",
    "teacher_model = BeitModel.from_pretrained(\"microsoft/beit-base-patch16-224\").to('cuda')\n",
    "teacher_processor = BeitFeatureExtractor.from_pretrained(\"microsoft/beit-base-patch16-224\")\n",
    "\n",
    "student_output_dim = 256\n",
    "beit_teacher_output_dim = 768\n",
    "num_classes = 80  # Number of COCO categories\n",
    "student_model = SimpleProjectionModel(vision_dim=beit_teacher_output_dim, num_classes=num_classes)\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "# Train the student model\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "prefix = \"attention_beit_coco_epoch_\"\n",
    "\n",
    "print(\"Training Student Model with COCO dataset...\")\n",
    "train_student_model_beit(\n",
    "    teacher_model, teacher_processor, student_model, train_loader, optimizer,\n",
    "    num_epochs=5, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Evaluate the student model\n",
    "print(\"Evaluating Student Model...\")\n",
    "evaluate_student_model_beit(\n",
    "    student_model, teacher_model, teacher_processor, val_loader, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa5f41-788b-49b7-a384-638348fdb0ff",
   "metadata": {},
   "source": [
    "<h4>10 epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9feacfc-ff1b-44f6-b8d7-a724a934b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Student Model with COCO dataset...\")\n",
    "train_student_model_beit(\n",
    "    teacher_model, teacher_processor, student_model, train_loader, optimizer,\n",
    "    num_epochs=10, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Evaluate the student model\n",
    "print(\"Evaluating Student Model...\")\n",
    "evaluate_student_model_beit(\n",
    "    student_model, teacher_model, teacher_processor, val_loader, checkpoint_dir=checkpoint_dir, prefix=prefix\n",
    ")\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1361cc-a32f-4b15-beff-894d3e217a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e4c6d-7d4e-4d49-980e-c4903eb8152b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe699b0-173b-4735-ab8d-4b3f49cc0cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
